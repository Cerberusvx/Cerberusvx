import pyttsx3
import speech_recognition as sr
import datetime
import wikipedia
import webbrowser
import os
import random
import smtplib
import requests
import json
import time
import math
from bs4 import BeautifulSoup
import torch
from transformers import GPTNeoForCausalLM, GPT2Tokenizer
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.image import MIMEImage
import openai_secret_manager
from github import Github




# Import ROCm for GPU acceleration
import tensorflow as tf
tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)
os.environ["TF_GPU_THREAD_MODE"] = "gpu_private"


# Get email account details from the system environment variables
secrets = openai_secret_manager.get_secret("google")
EMAIL_ADDRESS = secrets["email"]
EMAIL_PASSWORD = secrets["password"]


# Path to your local fork of EleutherAI/gpt-neo-2.7B
local_data_path = '/path/to/your/local/fork'


# Initialize tokenizer and model with your local data
tokenizer = GPT2Tokenizer.from_pretrained(local_data_path)
model = GPTNeoForCausalLM.from_pretrained(local_data_path)


# Set up the text-to-speech engine
engine = pyttsx3.init('sapi5')
voices = engine.getProperty('voices')
engine.setProperty('voice', voices[0].id)
engine.setProperty('rate', 175)


# create a recognizer instance
r = sr.Recognizer()


# set the AI's name
AI_NAME = "Ava"


def speak(audio):
    engine.say(audio)
    engine.runAndWait()


def wish_me():
    hour = datetime.datetime.now().hour
    if 0 <= hour < 12:
        speak("Good Morning!")
    elif 12 <= hour < 18:
        speak("Good Afternoon!")
    else:
        speak("Good Evening!")
    speak(f"I am {AI_NAME}. Please tell me how may I help you")


def take_command():
    with sr.Microphone() as source:
        print("Listening...")
        r.pause_threshold = 1
        audio = r.listen(source)
    try:
        print("Recognizing...")
        query = r.recognize_google(audio, language='en-in')
        print(f"User said: {query}")
    except Exception as e:
        print("Say that again please...")
        return "None"
    return query.lower()


def generate_text(prompt):
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    output = model.generate(input_ids, max_length=175, do_sample=True, top_k=50)
    return tokenizer.decode(output[0], skip_special_tokens=True)


def send_email(body, image):
    message = MIMEMultipart()
    message['From'] = EMAIL_ADDRESS
    receiver_email = input('Enter receiver email address: ')
    message['To'] = receiver_email
    message['Subject'] = 'Daily Inspiration'
    text = MIMEText(body)
    message.attach(text)
    with open(image, 'rb') as f:
        img_data = f.read()
    image = MIMEImage(img_data, name='inspiration.jpg')
    message.attach(image)
    try:
        with smtplib.SMTP('smtp.gmail.com', 587) as server:
            server.starttls()
            server.login(EMAIL_ADDRESS, EMAIL_PASSWORD)
            server.sendmail(EMAIL_ADDRESS, receiver_email, message.as_string())
        print('Email sent successfully')
    except Exception as e:
        print(f'Error sending email: {e}')




if __name__ == "__main__":
    wish_me()
    while True:
        query = take_command()


        if f"{AI_NAME.lower()} wikipedia" in query:
            speak('Searching Wikipedia...')
            query = query.replace(f"{AI_NAME.lower()} wikipedia", "")
            results = wikipedia.summary(query, sentences=2)
            speak("According to Wikipedia")
            print(results)
            speak(results)
 
        elif f"{AI_NAME.lower()} open youtube" in query:
            webbrowser.open("youtube.com")


        elif f"{AI_NAME.lower()} open google" in query:
            webbrowser.open("google.com")


        elif f"{AI_NAME.lower()} open stack overflow" in query:
            webbrowser.open("stackoverflow.com")


# Define the URL to scrape
base_url = "https://en.wikipedia.org"
random.seed(42)


# Define the URLs to scrape
base_url = "https://en.wikipedia.org"
ai_titles = [
    "Artificial_intelligence",
    "Machine_learning",
    "Deep_learning",
    "Neural_network",
    "Robotics",
    "Computer_vision",
    "Natural_language_processing"
]
comp_titles = [
    "Computer_science",
    "Computer",
    "Operating_system",
    "Programming_language",
    "Software_engineering",
    "Database",
    "Data_structure",
    "Algorithm"
]
science_titles = [
    "Science",
    "Physics",
    "Chemistry",
    "Biology",
    "Astronomy",
    "Geology",
    "Engineering"
]


# Set up scheduler to run 20 times per day for each topic
def run_daily_scrape():
    num_pages = 20
    for i in range(num_pages):
        # Scrape AI topics
        for title in ai_titles:
            page_url = base_url + "/wiki/" + title
            print("Scraping page:", page_url)
            # Make request to page
            page = requests.get(page_url)
            soup = BeautifulSoup(page.content, "html.parser")
            # Process page content here
            # ...
            # Wait for some time before making the next request
            time.sleep(5)
        # Scrape computer topics
        for title in comp_titles:
            page_url = base_url + "/wiki/" + title
            print("Scraping page:", page_url)
            # Make request to page
            page = requests.get(page_url)
            soup = BeautifulSoup(page.content, "html.parser")
            # Process page content here
            # ...
            # Wait for some time before making the next request
            time.sleep(5)
        # Scrape science topics
        for title in science_titles:
            page_url = base_url + "/wiki/" + title
            print("Scraping page:", page_url)
            # Make request to page
            page = requests.get(page_url)
            soup = BeautifulSoup(page.content, "html.parser")
            # Process page content here
            # ...
            # Wait for some time before making the next request
            time.sleep(5)


run_daily_scrape()


# replace the access_token with your own GitHub API access token
access_token = "<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>"
g = Github(access_token)


# replace with the name of your fork repository
fork_repo_name = "my_fork_repo"


# replace with the directory where you want to save the new code
new_code_dir = "new_code"


# get your fork repository object
fork_repo = g.get_user().get_repo(fork_repo_name)


# search for repositories with the keyword "python"
query = "python AI"
repos = g.search_repositories(query)


# extract the code files from the top 10 repositories
for repo in repos[:10]:
    # iterate through the code files in the repository
    for file in repo.get_contents(""):
        # extract the file content and save it to a file in your local fork repo
        if file.name.endswith(".py"):
            content = file.decoded_content.decode()
            file_path = os.path.join(fork_repo_name, new_code_dir, repo.name, file.path)
            file_dir = os.path.dirname(file_path)
            if not os.path.exists(file_dir):
                os.makedirs(file_dir)
            with open(file_path, "w") as f:
                f.write(content)
            # introduce a delay of 2 seconds between API requests
            time.sleep(2)


# Fine-tune the model on the extracted code
for repo in repos[:10]:
    for file in repo.get_contents(""):
        if file.name.endswith(".py"):
            content = file.decoded_content.decode()
            input_ids = tokenizer.encode(content, return_tensors="pt")
            model.train()
            loss = model(input_ids, labels=input_ids)[0]
            loss.backward()
            # introduce a delay of 2 seconds between API requests
            time.sleep(2)

